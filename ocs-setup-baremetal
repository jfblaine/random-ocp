~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
!!!!!!!!!!!!!! Take etcd backup !!!!!!!!!!!!!!
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# label the ocs nodes
OCP_DOMAIN=ocp4poc.example.com && \
oc label node ocs-0.${OCP_DOMAIN} cluster.ocs.openshift.io/openshift-storage='' --overwrite && \
oc label node ocs-1.${OCP_DOMAIN} cluster.ocs.openshift.io/openshift-storage='' --overwrite && \
oc label node ocs-2.${OCP_DOMAIN} cluster.ocs.openshift.io/openshift-storage='' --overwrite 

# && \
# oc adm taint nodes ocs-0.${OCP_DOMAIN} node.ocs.openshift.io/storage=true:NoSchedule && \
# oc adm taint nodes ocs-1.${OCP_DOMAIN} node.ocs.openshift.io/storage=true:NoSchedule && \
# oc adm taint nodes ocs-2.${OCP_DOMAIN} node.ocs.openshift.io/storage=true:NoSchedule

# oc get nodes -l cluster.ocs.openshift.io/openshift-storage -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}'

# remove label if needed
# OCP_DOMAIN=ocp4poc.example.com
# oc label node ocs-3.${OCP_DOMAIN} cluster.ocs.openshift.io/openshift-storage- --overwrite

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# to identify tainted nodes
# oc get nodes -o go-template='{{range $item := .items}}{{with $nodename := $item.metadata.name}}{{range $taint := $item.spec.taints}}{{if and (eq $taint.key "node.ocs.openshift.io/storage") (eq $taint.effect "NoSchedule")}}{{printf "%s\n" $nodename}}{{end}}{{end}}{{end}}{{end}}'

# remove taint
# oc adm taint nodes -l cluster.ocs.openshift.io/openshift-storage='' node.ocs.openshift.io/storage-

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create the local-storage ns and install the operator

echo '
---
apiVersion: v1
kind: Namespace
metadata:
  name: local-storage
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: local-operator-group
  namespace: local-storage
spec:
  targetNamespaces:
    - local-storage
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: local-storage-operator
  namespace: local-storage
spec:
  channel: "4.5" 
  installPlanApproval: Automatic
  name: local-storage-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace' | oc apply -f -

# run ansible playbook to create the localvolume
# based on device ids
# Note it is necessary to add the toleration
# to the localstorage CR if the taint was added
# to the ocs nodes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# create the openshift-storage namespace
# install ocs operator

echo '
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    openshift.io/cluster-monitoring: "true"
  name: openshift-storage
spec: {}
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-storage-operatorgroup
  namespace: openshift-storage
spec:
  serviceAccount:
    metadata:
      creationTimestamp: null
  targetNamespaces:
  - openshift-storage
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ocs-subscription
  namespace: openshift-storage
spec:
  channel: stable-4.4
  name: ocs-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace' | oc apply -f -

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create the storagecluster
echo '
---
apiVersion: ocs.openshift.io/v1
kind: StorageCluster
metadata:
  name: ocs-storagecluster
  namespace: openshift-storage
spec:
  manageNodes: false
  monDataDirHostPath: /var/lib/rook
  storageDeviceSets:
  - count: 1
    dataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 200Gi
        storageClassName: localblock
        volumeMode: Block
    name: ocs-deviceset
    placement: {}
    portable: false
    replica: 3
    resources: {}' | oc apply -f - -n openshift-storage
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
# more information here
# https://github.com/openshift/ocs-operator

oc patch storageclass ocs-storagecluster-cephfs -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' && \
oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"storage":{"pvc":{"claim":""}}}}' && \
oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"managementState":"Managed"}}' && \
sleep 15 && \
oc patch storageclass ocs-storagecluster-cephfs -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'

oc get pvc -n openshift-image-registry
oc get pod -n openshift-image-registry

# to create route for registry
oc patch configs.imageregistry.operator.openshift.io/cluster --patch '{"spec":{"defaultRoute":true}}' --type=merge

# to test registry storage
yum module install container-tools
OR
yum install podman

export REGISTRY_URL=default-route-openshift-image-registry.apps.ocp4poc.example.com
podman pull alpine
podman login -u $(oc whoami) -p $(oc whoami -t) $REGISTRY_URL --tls-verify=false
podman tag alpine $REGISTRY_URL/test/alpine
oc new-project test
podman push $REGISTRY_URL/test/alpine --tls-verify=false
oc get is -n test
oc delete project test

oc label nodes ocs-0.ocp4poc.example.com node-role.kubernetes.io/infra='' &&\
oc label nodes ocs-1.ocp4poc.example.com node-role.kubernetes.io/infra='' &&\
oc label nodes ocs-2.ocp4poc.example.com node-role.kubernetes.io/infra=''

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

to remove all default requests and limits:

apiVersion: ocs.openshift.io/v1
kind: StorageCluster
metadata:
  name: ocs-storagecluster
  namespace: openshift-storage
spec:
  manageNodes: false
  monDataDirHostPath: /var/lib/rook
  resources:      
    mon:      
      requests: {}
      limits: {}
    mds:
      requests: {}
      limits: {}
    rgw:      
      requests: {}
      limits: {}
    mgr:
      requests: {}
      limits: {}
    noobaa-core:      
      requests: {}
      limits: {}
    noobaa-db:        
      requests: {}
      limits: {}
  storageDeviceSets:
  - count: 1
    dataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100Gi
        storageClassName: 'localblock'
        volumeMode: Block
    name: ocs-deviceset
    placement: {}
    portable: true
    replica: 3
    resources:
      requests: {}
      limits: {}




